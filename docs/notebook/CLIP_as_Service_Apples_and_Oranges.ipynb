{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N9D7QbCmFLB"
      },
      "source": [
        "# Apples and Oranges\n",
        "\n",
        "<blockquote>Вы удивились бы, если бы вследствие каких-нибудь обстоятельств на яблонях и апельсинных деревьях вместо плодов вдруг выросли лягушки и ящерицы...\n",
        "\n",
        "*You would be surprised if, due to some circumstance, frogs and lizards suddenly grew on apple and orange trees instead of fruit...*\n",
        "\n",
        "Anton Chekov, *The Bet* (*Пари*), 1888</blockquote>\n",
        "\n",
        "<center><figure><img src=\"https://drive.google.com/uc?id=1AJARLkRsKVMKZ97S0vYXPKwQ401rVFSa\" width=\"550px\" alt=\"Pommes et oranges: Paul Cézanne, ca 1899\"/><figcaption><i>Pommes et oranges</i>: Paul Cézanne, ca 1899.</figcaption>\n",
        "</figure></center>\n",
        "\n",
        "“Comparing apples and oranges” is an idiom for comparing two things that are so different that they are not comparable, but are they really very different? Both are fruits, typically similar in sizes, both are widely eaten and generally enjoyed.  Truly incomparable things would be, for example, apples and jet liners, or oranges and the number 12.\n",
        "\n",
        "It's not trivially easy for a computer to tell the difference between pictures of apples and oranges, and even a basic AI task like this requires some sophisticated programming and machine learning.\n",
        " \n",
        "With [Jina's CLIP-as-service](https://clip-as-service.jina.ai/?utm_source=notebook-cas-apples), all the programming and training are already done! All you need to build a basic classifier is [Jina](https://jina.ai/?utm_source=notebook-cas-apples), a bit of Python, and your own natural language skills.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67VcLoatpwGZ"
      },
      "source": [
        "# Getting Started\n",
        "First, install [DocArray](https://docarray.jina.ai/?utm_source=notebook-cas-apples) and the [CLIP-as-service client](https://clip-as-service.jina.ai/user-guides/client/?utm_source=notebook-cas-apples).\n",
        "\n",
        "**⚠️ You _may_ be asked to \"Restart Runtime\" after this step. If you are, please click the button and restart the runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvJLRQ_gmB9a"
      },
      "outputs": [],
      "source": [
        "!pip install clip_client docarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsnzpIeEsuW0"
      },
      "source": [
        "**If you have access to the CaS demo server**, set the value of the variable `cas_server_url` to `grpcs://demo-cas.jina.ai:2096`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDmz379nqlsB"
      },
      "outputs": [],
      "source": [
        "cas_server_url = 'grpcs://demo-cas.jina.ai:2096'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9KWGBPOtfpI"
      },
      "source": [
        "**If you do not have access to the CaS demo server**, you can run CLIP-as-service in a Google Colab notebook by following the instructions on [this page](https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb).  Make sure you get the URL including port number of the server, as instructed on that page, and then come back here, uncomment the line below, and set the value of the variable `cas_server_url` to whatever URL you got from that notebook.  It should look like:\n",
        "\n",
        "```\n",
        "<one or two digits>.tcp.ngrok.io:<four or five digits>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmN52wA4uHxM"
      },
      "outputs": [],
      "source": [
        "# replace '6.tcp.ngrok.io:18096' with the server info from the other notebook,\n",
        "# then uncomment and run the line below:\n",
        "# cas_server_url = 'grpc://6.tcp.ngrok.io:18096' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AbipMugypcn"
      },
      "source": [
        "Finally, create a CLIP-as-service client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q_NcWqRzB3J"
      },
      "outputs": [],
      "source": [
        "from clip_client import Client\n",
        "\n",
        "client = Client(server=cas_server_url, asyncio=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jokf-f0uSAL"
      },
      "source": [
        "# Download data\n",
        "\n",
        "We have prepared a dataset consisting of photos of apples and oranges from the [Fruits and Vegetables Image Recognition Dataset on Kaggle](https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition?utm_source=notebook-cas-apples).\n",
        "\n",
        "You can download this directly from Jina Hub into a [DocumentArray](https://docarray.jina.ai/fundamentals/documentarray/index.html?utm_source=notebook-cas-apples) object like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOxYZTaq1KLQ"
      },
      "outputs": [],
      "source": [
        "from docarray import DocumentArray, Document\n",
        "\n",
        "apples_and_oranges = DocumentArray.pull(\"apples_and_oranges\", show_progress=True)\n",
        "print(\"Done loading!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zOSCcE7oGII"
      },
      "source": [
        "The entire dataset is 143 MB and may take up to a minute to download.\n",
        "\n",
        "A [DocumentArray](https://docarray.jina.ai/?utm_source=notebook-cas-apples) is a container for different kinds of objects and acts much like a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ong6n7VzoMQW",
        "outputId": "5412be41-2b59-4cbc-e5f9-38cc07b7c2de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(apples_and_oranges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFrNghz379nK"
      },
      "source": [
        "This should be 176 documents, each containing an image of one or more apples or oranges and a label that is either 'apple' or 'orange'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKFNzBE1om7h"
      },
      "source": [
        "Let's set up some code to look at the individual images stored in the `DocumentArray` object `apples_and_oranges`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2lS4pRJ1VLm"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "def show_image(doc):\n",
        "  fmt = None\n",
        "  if doc.mime_type and doc.mime_type.startswith(\"image/\"):\n",
        "    fmt = doc.mime_type[6:].strip()\n",
        "  display(Image(data=doc.blob, width=250, format=fmt))\n",
        "  print(f\"Label: {doc.tags['value']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-NXUobo4II"
      },
      "source": [
        "The function `show_image()` lets you look at the contents of `apples_and_oranges`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN8j_e2g72g8"
      },
      "outputs": [],
      "source": [
        "show_image(apples_and_oranges[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3woNBOrEMBW"
      },
      "outputs": [],
      "source": [
        "show_image(apples_and_oranges[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6u9bqTrted0"
      },
      "source": [
        "# Using CLIP-as-service to classify images\n",
        "\n",
        "We can use the CLIP-as-service to identify which images are of apples and which are of oranges, without any complex machine learning programming.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBaXzouuFKE"
      },
      "source": [
        "CLIP-as-service is able to compare images to text statements, and decide which statement best characterises the image.\n",
        "\n",
        "Let's take one image as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98kr5Vm2tH27"
      },
      "outputs": [],
      "source": [
        "pic = apples_and_oranges[52]\n",
        "show_image(pic) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwV_Mh6GvBJH"
      },
      "source": [
        "You should see a picture of an orange above.\n",
        "\n",
        "Now, lets construct a query against the CLIP-as-service server.  We will ask it to decide which of the following statements best describes this image:\n",
        "\n",
        "* *This is an orange.*\n",
        "* *This is an apple.*\n",
        "\n",
        "CLIP-as-service queries take the form of a [Document](https://docarray.jina.ai/fundamentals/document/?utm_source=notebook-cas-apples) object from the `DocumentArray` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxn252hCug48"
      },
      "outputs": [],
      "source": [
        "from docarray import Document\n",
        "\n",
        "query = Document(\n",
        "    blob=pic.blob,\n",
        "    matches=[\n",
        "        Document(text=\"This is an apple\"),\n",
        "        Document(text=\"This is an orange\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z53DwSKJwu8e"
      },
      "source": [
        "Then, we send the query to the CLIP-as-service server, using the client we created before.  It returns a `DocumentArray` object, containing one `Document` for each document we transmitted to the server.\n",
        "\n",
        "Since we only sent one `Document` to the server, we only get one `Document` back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGKpJRRKwunG"
      },
      "outputs": [],
      "source": [
        "result = client.rank([query])\n",
        "print(len(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVqKiK_hxydM"
      },
      "source": [
        "We can see in the `Document` object that score information has been attached to each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nar9a0BX08yb"
      },
      "outputs": [],
      "source": [
        "result[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0UFk_q91CRf"
      },
      "source": [
        "Let's define a function to tabulate this into an easier-to-read result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpg4R6zS1Bmw"
      },
      "outputs": [],
      "source": [
        "def show_scores(result_document):\n",
        "  print(\"Score     \\tText\")\n",
        "  print(\"----------\\t----------\")\n",
        "  for proposed_match in result_document.matches:\n",
        "    print(\"{:.8f}\".format(proposed_match.scores['clip_score'].value) + \"\\t\" + proposed_match.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xDGc6yB1b4m"
      },
      "source": [
        "Now let's apply it to the result we got from CLIP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVMidTQtwpDc"
      },
      "outputs": [],
      "source": [
        "show_scores(result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkCm5uwc0KV_"
      },
      "source": [
        "We can see that CLIP is clearly indicating that \"This is an orange\" better characterises this image than \"This is an apple\" does.\n",
        "\n",
        "In this case, it works about as well if we dispense with the \"this is a...\" part of the query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si2FqR_G0DAo"
      },
      "outputs": [],
      "source": [
        "query = Document(\n",
        "    blob=pic.blob,\n",
        "    matches=[\n",
        "        Document(text=\"apple\"),\n",
        "        Document(text=\"orange\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "result = client.rank([query])\n",
        "\n",
        "show_scores(result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yrrTSTG50DT"
      },
      "source": [
        "We can do this for any image in `apples_and_oranges`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj_UT9eu2To-"
      },
      "outputs": [],
      "source": [
        "def is_it_apple_or_orange(doc):\n",
        "  query = Document(\n",
        "    blob=doc.blob,\n",
        "    matches=[\n",
        "        Document(text=\"apple\"),\n",
        "        Document(text=\"orange\"),\n",
        "    ],\n",
        "  )\n",
        "  result = client.rank([query])\n",
        "  best_match = result[0].matches[0] # the matches are reordered from best to worst by the server\n",
        "  show_image(doc)\n",
        "  print(f\"Best match: {best_match.text}\")\n",
        "  print(f\"Score: {best_match.scores['clip_score'].value}\")\n",
        "\n",
        "is_it_apple_or_orange(apples_and_oranges[12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cVep27G7eV7"
      },
      "source": [
        "Try it with other images from `apples_and_oranges`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYBnR2Kd7KwV"
      },
      "outputs": [],
      "source": [
        "is_it_apple_or_orange(apples_and_oranges[58])\n",
        "is_it_apple_or_orange(apples_and_oranges[23])\n",
        "is_it_apple_or_orange(apples_and_oranges[31])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROShLfXaAIku"
      },
      "source": [
        "**Accuracy of CLIP**\n",
        "\n",
        "We can check how well CLIP can tell the difference between apples and oranges by processing all of the images in `apples_and_oranges`.\n",
        "\n",
        "Let's create a function to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ndki-59p90t"
      },
      "outputs": [],
      "source": [
        "def clip_classifier(data, matches):\n",
        "  # first we break the data into batches of ten\n",
        "  batches = [data[i:i+10] for i in range(0, len(data), 10)]\n",
        "\n",
        "  label_predictions = []\n",
        "  errors = []\n",
        "\n",
        "  # we iterate over the batches, sending them to CLIP-as-service\n",
        "  for batch_num, batch in enumerate(batches):\n",
        "    print(f\"Processing batch #{batch_num}. Errors so far: {len(errors)}\")\n",
        "    queries = []\n",
        "    for doc in batch:\n",
        "      queries.append(\n",
        "          Document(\n",
        "              blob=doc.blob,\n",
        "              tags=doc.tags,\n",
        "              matches=[Document(text=m) for m in matches],\n",
        "          )\n",
        "      )\n",
        "    results = client.rank(queries)\n",
        "    for result_doc in results:\n",
        "      label_predictions.append((result_doc.tags['value'], result_doc.matches[0].text))\n",
        "      if result_doc.tags['value'] !=  result_doc.matches[0].text:\n",
        "        errors.append(result_doc)\n",
        "\n",
        "  print(f\"Total processed: {len(label_predictions)}\")\n",
        "  print(f\"Errors: {len(errors)}\")\n",
        "  return label_predictions, errors\n",
        "\n",
        "labels, errors = clip_classifier(apples_and_oranges, [\"apple\", \"orange\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfqKdh3F5oM"
      },
      "source": [
        "You should see **1** error out of 176, or a raw accuracy of 99.4%.  This is the CLIP model without fine-tuning, so you can see that it can be used a low-error object identifier in many cases without any training or programming.\n",
        "\n",
        "CLIP-as-service enables you to integrate this in any Python program without any specialized AI or machine learning knowledge: Just the Jina AI client installation, general Python, and the English language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcs8qXRjGQLP"
      },
      "source": [
        "Let's take a look at the error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "GIQy1oHfDB7H",
        "outputId": "a2b5b225-7b0a-4995-b252-87a25ecf4952"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: orange\n"
          ]
        }
      ],
      "source": [
        "show_image(errors[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Okjc3KzcVK"
      },
      "source": [
        "On visual inspection, you can see how this might be mistaken for a green apple at first glance even by a human."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqOB9FiG7-Lz"
      },
      "source": [
        "**Using other images**\n",
        "\n",
        "You can even try it with your own images.  Just provide a URI to access an image via the Internet.  If you are running this in a local notebook instead of Colab, you can provide a file path as URI to process images you have locally stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvwpGjjG7rnh"
      },
      "outputs": [],
      "source": [
        "def test_image_uri(user_uri, texts):\n",
        "  query = Document(\n",
        "    uri=user_uri,\n",
        "    matches=[\n",
        "        Document(text=text)\n",
        "        for text in texts\n",
        "    ],\n",
        "  )\n",
        "  result = client.rank([query])\n",
        "  best_match = result[0].matches[0] # the matches are reordered from best to worst by the server\n",
        "  display(Image(url=user_uri, width=250))\n",
        "  print(f\"Best match: {best_match.text}\")\n",
        "  print(f\"Score: {best_match.scores['clip_score'].value}\")\n",
        "\n",
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/a/af/5_apples.jpg\", [\"apple\", \"orange\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAD5dpEu9Eod"
      },
      "source": [
        "Try it with other pictures of apples and oranges.  Please don't use excessively large images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vmq07o18m77"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/3/37/Gammel_dame_kj%C3%B8per_epler.JPG\", [\"apple\", \"orange\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAmWCUTq_Fz6"
      },
      "source": [
        "Now, let's see what happens if we give CLIP-as-service and image with both an apple and an orange in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbUVJroP8uMy"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/a/a8/Apple_and_Orange_-_they_do_not_compare.jpg\", [\"apple\", \"orange\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bC7d6rI_NcE"
      },
      "source": [
        "You can see that forcing CLIP to answer with either \"apple\" or \"orange\" yields a low score when both apply.  No matter what we give it, it **has to** choose one as more representative of the image than the other.\n",
        "\n",
        "If a picture has neither apples nor oranges in it, the result is very unreliable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkxZBsKkLaw2"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/5/55/Banana_%281%29.jpg\", [\"apple\", \"orange\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF7aeeVU0J9Y"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/\"\n",
        "\"Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/515px-Mona_\"\n",
        "\"Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\", [\"apple\", \"orange\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfbUvYFeM5_1"
      },
      "source": [
        "**It is very important to understand that CLIP queries will always select the best fitting of the options you give it.  If none of those options fits, the results will be somewhat random.**\n",
        "\n",
        "One way to address this is to try to use polar sentences for matching.  For example, 'apple' and 'not apple':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hDOvlzqMwoB"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/a/af/5_apples.jpg\", [\"apple\", \"not apple\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv8U4eduNpSr"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/9/92/ATIIIII.jpg\", [\"apple\", \"not apple\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-FVjQcXOLIf"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/5/55/Banana_%281%29.jpg\", [\"apple\", \"not apple\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgYXoukU0q7O"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci\"\n",
        "\"%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\", [\"apple\", \"not apple\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgZU2VL2Rr_O"
      },
      "source": [
        "Let's see how accurate it is with the entire apples and oranges dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyS2po90Rrbj"
      },
      "outputs": [],
      "source": [
        "# correct the tags to the new class labels\n",
        "apples_and_not_apples = DocumentArray(\n",
        "    [\n",
        "        Document(\n",
        "            blob=doc.blob,\n",
        "            tags={'value' : 'apple' if doc.tags['value'] == 'apple' else 'not apple'}\n",
        "        )\n",
        "        for doc in apples_and_oranges\n",
        "    ]\n",
        ")\n",
        "\n",
        "labels, errors = clip_classifier(apples_and_not_apples, [\"apple\", \"not apple\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvoZJcUMUA25"
      },
      "source": [
        "You can see there are a lot errors! This classifier is only 86.3% accurate.\n",
        "\n",
        "Let's count the errors per class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ5Xy2SOnjKQ",
        "outputId": "80b1d56a-2b0d-43ce-9066-8d72868791a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apple errors:\t23 out of 88\n",
            "orange errors:\t1 out of 88\n"
          ]
        }
      ],
      "source": [
        "total_counts = {'apple':0, 'not apple':0}\n",
        "for doc in apples_and_not_apples:\n",
        "  total_counts[doc.tags['value']] += 1\n",
        "\n",
        "error_counts = {'apple':0, 'not apple':0}\n",
        "for doc in errors:\n",
        "  error_counts[doc.tags['value']] += 1\n",
        "print(f\"apple errors:\\t{error_counts['apple']} out of {total_counts['apple']}\")\n",
        "print(f\"orange errors:\\t{error_counts['not apple']} out of {total_counts['not apple']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOReLrxq-oqb"
      },
      "source": [
        "The errors are very asymetric. Only one orange was identified as an apple, but many apples were identified as 'not apple'.\n",
        "\n",
        "This is because the text 'apple' and 'not apple' are very similar from the perspective of the neural network.  **CLIP does not handle negation the way people do, and it's much more reliable to give it positive statements to identify with.**\n",
        "\n",
        "Instead of using 'not', let's try a different pair of contrasts: 'apple' and 'fruit':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp1jRt-d6zN9"
      },
      "outputs": [],
      "source": [
        "# correct the tags to the new class labels\n",
        "apples_and_fruit = DocumentArray(\n",
        "    [\n",
        "        Document(\n",
        "            blob=doc.blob,\n",
        "            tags={'value' : 'apple' if doc.tags['value'] == 'apple' else 'fruit'}\n",
        "        )\n",
        "        for doc in apples_and_oranges\n",
        "    ]\n",
        ")\n",
        "\n",
        "labels, errors = clip_classifier(apples_and_fruit, [\"apple\", \"fruit\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoZnAsujmIdu"
      },
      "source": [
        "Using this constrast, there are far fewer errors.  Accuracy is 97.2%.  Pictures of oranges match \"fruit\" better than \"not apple\".\n",
        "\n",
        "Let's look at the errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QCTB0f_5UwEB"
      },
      "outputs": [],
      "source": [
        "for error in errors:\n",
        "  show_image(error)\n",
        "  print(f\"Predicted label: {error.matches[0].text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcldMkTL0sjD"
      },
      "source": [
        "Most of the errors are apples that match \"fruit\" better than \"apple\", according to CLIP, and one very anomalous orange.\n",
        "\n",
        "But we still have the problem that any picture that's not fruit at all is still going to be labeled as either an apple or some kind of fruit.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "TOJPncvQ2ZrS",
        "outputId": "b1b64c31-b370-47c4-e7bc-fbd253ce735f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\" width=\"250\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best match: fruit\n",
            "Score: 0.5066827535629272\n"
          ]
        }
      ],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci\"\n",
        "\"%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\", [\"apple\", \"fruit\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQsMJuP32wa7"
      },
      "source": [
        "We can resolve this by adding different \"control\" words that describe images more generally.  Like \"picture\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGMhhCi29kN"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci\"\n",
        "\"%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\", [\"apple\", \"picture\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw8mY4WS3DRm"
      },
      "source": [
        "Let's try it with the apple and orange pictures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijf7MUGZ0gqU"
      },
      "outputs": [],
      "source": [
        "# correct the tags to the new class labels\n",
        "apples_and_other = DocumentArray(\n",
        "    [\n",
        "        Document(\n",
        "            blob=doc.blob,\n",
        "            tags={'value' : 'apple' if doc.tags['value'] == 'apple' else 'picture'}\n",
        "        )\n",
        "        for doc in apples_and_oranges\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "labels, errors = clip_classifier(apples_and_other, [\"apple\", \"picture\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXPCHphA3ddu"
      },
      "source": [
        "Using \"picture\" instead of \"fruit\" to contrast to \"apple\" produces a lot more errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IoMbGMJ21f_A"
      },
      "outputs": [],
      "source": [
        "for error in errors:\n",
        "  show_image(error)\n",
        "  print(f\"Predicted label: {error.matches[0].text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkg7pC6Z3umM"
      },
      "source": [
        "Quite a few pictures of oranges match \"apple\" better than \"picture\", according to CLIP. But what happens if we contrast \"apple\" with BOTH \"picture\" and \"fruit\"?\n",
        "\n",
        "To do this, we need to modify the `clip_classifier` method to just test for apples and ignore other categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNtXxI-869Oq"
      },
      "outputs": [],
      "source": [
        "def apple_classifier(data, matches):\n",
        "  # first we break the data into batches of ten\n",
        "  batches = [data[i:i+10] for i in range(0, len(data), 10)]\n",
        "\n",
        "  label_predictions = []\n",
        "  errors = []\n",
        "\n",
        "  # we iterate over the batches, sending them to CLIP-as-service\n",
        "  for batch_num, batch in enumerate(batches):\n",
        "    print(f\"Processing batch #{batch_num}. Errors so far: {len(errors)}\")\n",
        "    queries = []\n",
        "    for doc in batch:\n",
        "      queries.append(\n",
        "          Document(\n",
        "              blob=doc.blob,\n",
        "              tags=doc.tags,\n",
        "              matches=[Document(text=m) for m in matches],\n",
        "          )\n",
        "      )\n",
        "    results = client.rank(queries)\n",
        "    for result_doc in results:\n",
        "      label_predictions.append((result_doc.tags['value'], result_doc.matches[0].text))\n",
        "      if result_doc.tags['value'] ==  \"apple\" and result_doc.matches[0].text != \"apple\":\n",
        "        errors.append(result_doc)\n",
        "    if result_doc.tags['value'] !=  \"apple\" and result_doc.matches[0].text == \"apple\":\n",
        "        errors.append(result_doc)\n",
        "        \n",
        "  print(f\"Total processed: {len(label_predictions)}\")\n",
        "  print(f\"Errors: {len(errors)}\")\n",
        "  return label_predictions, errors\n",
        "\n",
        "labels, errors = apple_classifier(apples_and_oranges, [\"apple\", \"fruit\", \"picture\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SXh9VSU6WKt"
      },
      "source": [
        "This reduces the errors by quite a lot compared to contrasting \"apple\" with \"picture\", but only a little bit compared to \"apple\" and \"fruit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBvGC5w50eV"
      },
      "outputs": [],
      "source": [
        "for error in errors:\n",
        "  show_image(error)\n",
        "  print(f\"Predicted label: {error.matches[0].text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O087B4Jef4z"
      },
      "source": [
        "However, we can reasonably reliably filter out images that are neither \"fruit\" nor \"apple\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CmyNydW6ILe"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci\"\n",
        "\"%2C_from_C2RMF_retouched.jpg/515px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\", [\"apple\", \"fruit\", \"picture\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKp5TdfDevPV"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Macropodus_opercularis_-_side_%28aka%29.jpg\", [\"apple\", \"fruit\", \"picture\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWfV-lp1fpss"
      },
      "outputs": [],
      "source": [
        "test_image_uri(\"https://upload.wikimedia.org/wikipedia/commons/b/bd/Spaghetti_%284623718139%29.jpg\", [\"apple\", \"fruit\", \"picture\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqg8lF5chZ_R"
      },
      "source": [
        "By experimenting with the contrasting texts you use, you can use CLIP-as-service to get very good image classification results **without any programming at all**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQaXtOWN7LXN"
      },
      "source": [
        "# Powerful features for image analysis\n",
        "\n",
        "In addition to classifying scenes and objects in images, CLIP-as-service can analyse the number of items in an image and their relative locations. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7u2n-o5xNBv"
      },
      "outputs": [],
      "source": [
        "# If you ran the previous section, you can skip this part\n",
        "from docarray import DocumentArray, Document\n",
        "from IPython.display import Image\n",
        "\n",
        "# This function is already defined if you ran the previous section\n",
        "def show_scores(result_document):\n",
        "  print(\"Score     \\tText\")\n",
        "  print(\"----------\\t----------\")\n",
        "  for proposed_match in result_document.matches:\n",
        "    print(\"{:.8f}\".format(proposed_match.scores['clip_score'].value) + \"\\t\" + proposed_match.text)\n",
        "\n",
        "def show_image(doc):\n",
        "  fmt = None\n",
        "  if doc.mime_type and doc.mime_type.startswith(\"image/\"):\n",
        "    fmt = doc.mime_type[6:].strip()\n",
        "  display(Image(data=doc.blob, width=250, format=fmt))\n",
        "  print(f\"Label: {doc.tags['value']}\")\n",
        "\n",
        "apples_and_oranges = DocumentArray.pull(\"apples_and_oranges\", show_progress=True)\n",
        "print(\"Done loading!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diMls_TWxIQA"
      },
      "source": [
        "## Counting objects\n",
        "\n",
        "You can use CLIP-as-service to count objects in images using natural language. Let's define a function to do so by using the same query interface used above to tell apples from oranges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbvNCD8M7jGe"
      },
      "outputs": [],
      "source": [
        "def count_apples(image):\n",
        "    query = Document(\n",
        "        blob=image.blob,\n",
        "        matches=[\n",
        "            Document(text=\"orange\"),\n",
        "            Document(text=\"one apple\"),\n",
        "            Document(text=\"two apples\"),\n",
        "            Document(text=\"three apples\"),\n",
        "            Document(text=\"four or more apples\"),\n",
        "        ],\n",
        "        tags=image.tags\n",
        "    )\n",
        "    result = client.rank([query])\n",
        "    show_image(query)\n",
        "    print()\n",
        "    show_scores(result[0])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQoUsUqyAxm-"
      },
      "source": [
        "Let's test this on images with more than one apple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7tfFvzHAxN9"
      },
      "outputs": [],
      "source": [
        "count_apples(apples_and_oranges[0])\n",
        "count_apples(apples_and_oranges[32])\n",
        "count_apples(apples_and_oranges[172])\n",
        "count_apples(apples_and_oranges[173])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4sVsar9hxDs"
      },
      "source": [
        "This approach to counting becomes unreliable with more than three or four things to count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx_ra0csh4CT"
      },
      "source": [
        "## Spatial Relations\n",
        "\n",
        "CLIP-as-service can also detect some spatial relationships.  For example, is the apple in front of or behind the grapes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dO4Y1vnjs0w"
      },
      "outputs": [],
      "source": [
        "image = Document(uri=\"https://upload.wikimedia.org/wikipedia/commons/0/00/Grapes_and_apple.jpg\", tags={'value': 'label'})\n",
        "image.load_uri_to_blob()\n",
        "\n",
        "query = Document(\n",
        "    blob=image.blob,\n",
        "    matches=[\n",
        "        Document(text=\"the apple is in front of the grapes\"),\n",
        "        Document(text=\"the apple is behind the grapes\"),\n",
        "        \n",
        "    ],\n",
        "    tags=image.tags\n",
        ")\n",
        "result = client.rank([query])\n",
        "display(Image(data=query.blob, width=250))\n",
        "print()\n",
        "show_scores(result[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnWgPQuKwbu6"
      },
      "outputs": [],
      "source": [
        "image = Document(uri=\"https://upload.wikimedia.org/wikipedia/commons/f/f5/2016-09-15_04-25-35_recup.jpg\", tags={'value': 'label'})\n",
        "image.load_uri_to_blob()\n",
        "\n",
        "query = Document(\n",
        "    blob=image.blob,\n",
        "    matches=[\n",
        "        Document(text=\"the apple is between the pears and the banana\"),\n",
        "        Document(text=\"the bananas are between the apple and the pears\"),\n",
        "        \n",
        "    ],\n",
        "    tags=image.tags\n",
        ")\n",
        "result = client.rank([query])\n",
        "display(Image(data=query.blob, width=250))\n",
        "print()\n",
        "show_scores(result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NObEzAA5k892"
      },
      "source": [
        "# Using CLIP-as-service to do local image search\n",
        "\n",
        "Another out-of-the-box application of CLIP-as-service is perform searches on locally stored image data, accessing the server only to encode the image data and the queries, while using JINA locally to search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlOmSu1jtSVi"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you have done any of the previous sections\n",
        "from docarray import DocumentArray, Document\n",
        "from IPython.display import Image\n",
        "\n",
        "# This function is already defined if you ran the previous section\n",
        "def show_scores(result_document):\n",
        "  print(\"Score     \\tText\")\n",
        "  print(\"----------\\t----------\")\n",
        "  for proposed_match in result_document.matches:\n",
        "    print(\"{:.8f}\".format(proposed_match.scores['clip_score'].value) + \"\\t\" + proposed_match.text)\n",
        "\n",
        "def show_image(doc):\n",
        "  fmt = None\n",
        "  if doc.mime_type and doc.mime_type.startswith(\"image/\"):\n",
        "    fmt = doc.mime_type[6:].strip()\n",
        "  display(Image(data=doc.blob, width=250, format=fmt))\n",
        "  print(f\"Label: {doc.tags['value']}\")\n",
        "\n",
        "apples_and_oranges = DocumentArray.pull(\"apples_and_oranges\", show_progress=True)\n",
        "print(\"Done loading!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXqtonlby9Bj"
      },
      "source": [
        "CLIP works by encoding images and texts into vectors, using a trained neural network. We can then compare these encodings directly.\n",
        "\n",
        "First, we will encode all the images in `apples_and_oranges`.  This may take several minutes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lra0tVthVQm"
      },
      "outputs": [],
      "source": [
        "import time, jina\n",
        "\n",
        "def encode_apples_and_oranges():\n",
        "  encoded_apples_and_oranges = []\n",
        "  batchsize = 4\n",
        "  def _add(i, j):\n",
        "      if j >= 3:\n",
        "          raise Exception(\"Can't complete encoding due to server interruptions!\")\n",
        "      try:\n",
        "          encoded_apples_and_oranges.extend(client.encode(apples_and_oranges[i:i+batchsize]))\n",
        "      except jina.excepts.BadClient:\n",
        "          print(f\"fail {j+1}\")\n",
        "          time.sleep(5)\n",
        "          _add(i, j+1)\n",
        "          \n",
        "  for i in range(0, len(apples_and_oranges), batchsize):\n",
        "      _add(i, 0)\n",
        "      if i and i%10 == 0:\n",
        "        print(\"{:.2f}% done\".format(100.0*i/len(apples_and_oranges)))\n",
        "  \n",
        "  print(\"Done encoding!\")\n",
        "  return DocumentArray(encoded_apples_and_oranges)\n",
        "\n",
        "encoded_images = encode_apples_and_oranges()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b733KK1e1gf5"
      },
      "source": [
        "You can see that the server has assigned a vector to each image by looking at the `embedding` field in each member of `encoded_images`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGGbU_mLzedr"
      },
      "outputs": [],
      "source": [
        "encoded_images[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DUIwZrz2DBv"
      },
      "source": [
        "Now, let's use CLIP-as-service to encode a text query. In this case, for green apples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_naJpY4c1w5h",
        "outputId": "58302bf5-41f1-4df7-a5a3-52ce28ad4e19"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = client.encode([Document(text=\"green apple\")])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbi9wE02nS2"
      },
      "source": [
        "You can see that query also has an assigned vector in the `embedding` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SgJmh1u2P-G"
      },
      "outputs": [],
      "source": [
        "query.embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPAioSMg2zLj"
      },
      "source": [
        "Now we can use a local function in the `DocumentArray` object to find matches in `encoded_images` without invoking the CLIP-as-service server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx8LiEC42vId"
      },
      "outputs": [],
      "source": [
        "result = encoded_images.find(query, limit=5)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETPh4Zlr3LWW"
      },
      "source": [
        "The optional `limit` argument restricts the number of items returned. The result should be the same length as `limit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22U1f6vs3J4b",
        "outputId": "17a82d62-08a5-47fe-98a1-fc1b8defbbfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWlVLjov3pHQ"
      },
      "source": [
        "Let's look at the images found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzrr9f5Y3Vy_"
      },
      "outputs": [],
      "source": [
        "for found_image in result:\n",
        "  show_image(found_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giz2Rj4f4KcX"
      },
      "source": [
        "Or alternately, we could search for images of cut oranges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcoFATqo32da"
      },
      "outputs": [],
      "source": [
        "query = client.encode([Document(text=\"cut orange\")])[0]\n",
        "result = encoded_images.find(query, limit=5)[0]\n",
        "for found_image in result:\n",
        "  show_image(found_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuYUVbgI5upJ"
      },
      "source": [
        "Furthermore, you can use the encodings of images to search for similar images.\n",
        "\n",
        "Let's take an image of an orange from another source and encode it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mI3RDoV55mW"
      },
      "outputs": [],
      "source": [
        "outside_image = Document(uri=\"https://upload.wikimedia.org/wikipedia/commons/9/9e/Single_Orange_%28Fruit%29.jpg\", tags={'value': 'orange'})\n",
        "outside_image.load_uri_to_blob()\n",
        "show_image(outside_image)\n",
        "image_query = client.encode([outside_image])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1I_7bIL7YdZ"
      },
      "source": [
        "Now we're going to use that encoding as a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFLDasrr7eyF"
      },
      "outputs": [],
      "source": [
        "result = encoded_images.find(image_query, limit=5)[0]\n",
        "for found_image in result:\n",
        "  show_image(found_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJpcNbf5XgG"
      },
      "source": [
        "Using CLIP-as-service this way, you search local image repositories, using texts or images as queries. The service encodes locally stored objects just once, and then encodes each new query, minimizing the number of calls to the service."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9N9D7QbCmFLB",
        "diMls_TWxIQA",
        "Vx_ra0csh4CT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}